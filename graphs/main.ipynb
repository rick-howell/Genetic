{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1561 graphs\n",
      "Graph 1\n",
      "Adjacency Matrix:\n",
      "tensor([[0, 1, 1, 0, 0, 1, 1],\n",
      "        [1, 0, 0, 1, 0, 1, 1],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 0, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]], dtype=torch.int8)\n",
      "Selected Invariants:\n",
      "tensor([ 1.0000,  7.0000,  0.0000,  0.6180, -1.7020,  6.0000,  4.0000, 16.0000,\n",
      "         7.0000])\n",
      "\n",
      "Graph 2\n",
      "Adjacency Matrix:\n",
      "tensor([[0, 0, 1, 1],\n",
      "        [0, 0, 1, 1],\n",
      "        [1, 1, 0, 1],\n",
      "        [1, 1, 1, 0]], dtype=torch.int8)\n",
      "Selected Invariants:\n",
      "tensor([ 0.0000,  4.0000,  1.0000,  0.0000, -1.5620,  3.0000,  2.0000,  5.0000,\n",
      "         4.0000])\n",
      "\n",
      "Graph 3\n",
      "Adjacency Matrix:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [1, 1, 0, 1, 0]], dtype=torch.int8)\n",
      "Selected Invariants:\n",
      "tensor([ 0.0000,  4.1700,  1.0000,  0.7650, -1.8480,  3.0000,  1.0000,  4.0000,\n",
      "         5.0000])\n",
      "\n",
      "Graph 4\n",
      "Adjacency Matrix:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.int8)\n",
      "Selected Invariants:\n",
      "tensor([ 0., 10.,  8.,  0., -3.,  9.,  1.,  9., 10.])\n",
      "\n",
      "Graph 5\n",
      "Adjacency Matrix:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.int8)\n",
      "Selected Invariants:\n",
      "tensor([ 1.0000,  8.0000,  1.0000,  0.8310, -1.9000,  7.0000,  1.0000, 17.0000,\n",
      "         8.0000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def parse_graph_data(file_path):\n",
    "    graphs = []\n",
    "    current_graph = {}\n",
    "    blank_line_count = 0\n",
    "    parsing_edges = True\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                blank_line_count += 1\n",
    "                if blank_line_count == 1:\n",
    "                    parsing_edges = False\n",
    "                elif blank_line_count == 3:\n",
    "                    if current_graph:\n",
    "                        graphs.append(current_graph)\n",
    "                        current_graph = {}\n",
    "                    blank_line_count = 0\n",
    "                    parsing_edges = True\n",
    "                continue\n",
    "            \n",
    "            blank_line_count = 0\n",
    "            \n",
    "            if parsing_edges:\n",
    "                vertex, neighbors = line.split(':')\n",
    "                if 'vertices' not in current_graph:\n",
    "                    current_graph['vertices'] = []\n",
    "                    current_graph['edges'] = []\n",
    "                vertex = int(vertex)\n",
    "                neighbors = list(map(int, neighbors.split()))\n",
    "                current_graph['vertices'].append(vertex)\n",
    "                for neighbor in neighbors:\n",
    "                    if {vertex, neighbor} not in current_graph['edges']:\n",
    "                        current_graph['edges'].append({vertex, neighbor})\n",
    "            else:\n",
    "                key, value = re.split(r':\\s*', line, 1)\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                current_graph[key] = value\n",
    "    \n",
    "    if current_graph:\n",
    "        graphs.append(current_graph)\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def create_adjacency_matrix(vertices, edges):\n",
    "    n = max(vertices)\n",
    "    matrix = np.zeros((n, n), dtype=np.int8)\n",
    "    for edge in edges:\n",
    "        i, j = list(edge)\n",
    "        matrix[i-1][j-1] = 1\n",
    "        matrix[j-1][i-1] = 1\n",
    "    return matrix\n",
    "\n",
    "def graph_to_tensor(graph):\n",
    "    vertices = graph['vertices']\n",
    "    edges = graph['edges']\n",
    "    adj_matrix = create_adjacency_matrix(vertices, edges)\n",
    "    \n",
    "    # Select the invariants we want to use\n",
    "    selected_invariants = [\n",
    "        'Genus',\n",
    "        'Laplacian Largest Eigenvalue',\n",
    "        'Number of Zero Eigenvalues',\n",
    "        'Second Largest Eigenvalue',\n",
    "        'Smallest Eigenvalue',\n",
    "        'Maximum Degree',\n",
    "        'Minimum Degree',\n",
    "        'Number of Edges',\n",
    "        'Number of Vertices',\n",
    "    ]\n",
    "    \n",
    "    invariants = [graph.get(inv, 0) for inv in selected_invariants]\n",
    "\n",
    "    # If the invariant is a string, it means it was most likely None in the file\n",
    "    # We'll need to remove the entire graph if any of the invariants are None\n",
    "    if any(isinstance(inv, str) for inv in invariants):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'adjacency': torch.tensor(adj_matrix, dtype=torch.int8),\n",
    "        'invariants': torch.tensor(invariants, dtype=torch.float32)\n",
    "    }\n",
    "\n",
    "def process_graphs(file_path):\n",
    "    graphs = parse_graph_data(file_path)\n",
    "    return [graph_to_tensor(graph) for graph in graphs]\n",
    "\n",
    "\n",
    "file_path = 'list_1561_graphs.txt'\n",
    "graphs = process_graphs(file_path)\n",
    "print(f\"Processed {len(graphs)} graphs\")\n",
    "\n",
    "\n",
    "for i, graph_data in enumerate(graphs[:5]):\n",
    "    print(f\"Graph {i + 1}\")\n",
    "    print(\"Adjacency Matrix:\")\n",
    "    print(graph_data['adjacency'])\n",
    "    print(\"Selected Invariants:\")\n",
    "    print(graph_data['invariants'])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.0000,  0.0000,  0.6180,  ...,  4.0000, 16.0000,  7.0000],\n",
      "        [ 4.0000,  1.0000,  0.0000,  ...,  2.0000,  5.0000,  4.0000],\n",
      "        [ 4.1700,  1.0000,  0.7650,  ...,  1.0000,  4.0000,  5.0000],\n",
      "        ...,\n",
      "        [ 6.0000,  2.0000,  1.4140,  ...,  2.0000, 12.0000,  7.0000],\n",
      "        [ 6.1730,  2.0000,  1.0460,  ...,  1.0000, 12.0000,  7.0000],\n",
      "        [ 8.0000,  0.0000,  1.0000,  ...,  5.0000, 24.0000,  8.0000]])\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 2.])\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the information to train the model\n",
    "\n",
    "# The invariants are set up such that the output is the first idx, and the input is the rest\n",
    "# We'll ignore the adjacency matrix for now\n",
    "\n",
    "# Let's first remove any graphs that have None invariants\n",
    "graphs = [graph for graph in graphs if graph is not None]\n",
    "X = torch.stack([graph['invariants'][1:] for graph in graphs])\n",
    "y = torch.stack([graph['invariants'][0] for graph in graphs])\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(max(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 0 - Loss: 0.09302996844053268\n",
      "Epoch 10 - Loss: 0.019907914102077484\n",
      "Epoch 20 - Loss: 0.012233015149831772\n",
      "Epoch 30 - Loss: 0.010720688849687576\n",
      "Epoch 40 - Loss: 0.009131488390266895\n",
      "Epoch 50 - Loss: 0.008348231203854084\n",
      "Epoch 60 - Loss: 0.007886018604040146\n",
      "Epoch 70 - Loss: 0.008138664998114109\n",
      "Epoch 80 - Loss: 0.008257956244051456\n",
      "Epoch 90 - Loss: 0.007510337512940168\n",
      "Predicting for input: tensor([ 9.2750,  4.0000,  1.0000, -3.0000,  8.0000,  2.0000, 26.0000, 10.0000])\n",
      "Expected output: tensor([0.2500])\n",
      "Predicted output: tensor([0.3960], grad_fn=<ViewBackward0>)\n",
      "\n",
      "__________________________\n",
      "\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.23%\n"
     ]
    }
   ],
   "source": [
    "# We'll now use a simple MLP model\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "NUM_FEATURES = X.shape[1]\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(NUM_FEATURES, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, NUM_CLASSES)\n",
    "        self.out = torch.nn.Linear(NUM_CLASSES, 1)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.nn.Tanh()(x) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We'll use a sigmoid activation function for the hidden layers\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self):\n",
    "        torch.save(self.state_dict(), 'mlp.dat')\n",
    "\n",
    "    def load_weights(self):\n",
    "        self.load_state_dict(torch.load('mlp.dat'))\n",
    "    \n",
    "model = MLP()\n",
    "print(model)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Let's split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "y_train = y_train.view(-1, 1) / max(y)\n",
    "y_test = y_test.view(-1, 1) / max(y)\n",
    "\n",
    "# Now we can train the model\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n",
    "\n",
    "model.save_weights()\n",
    "\n",
    "random_idx = np.random.randint(0, len(X_test))\n",
    "test_input = X_test[random_idx]\n",
    "\n",
    "print(f\"Predicting for input: {test_input}\")\n",
    "print(f\"Expected output: {y_test[random_idx]}\")\n",
    "print(f\"Predicted output: {model(test_input)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n__________________________\\n\")\n",
    "\n",
    "# Now we can test the model\n",
    "\n",
    "model = MLP()\n",
    "print(model)\n",
    "model.load_weights()\n",
    "model.eval()\n",
    "\n",
    "predicted = model(X_test) * max(y)\n",
    "actual = y_test * max(y)\n",
    "\n",
    "# Let's plot the predicted vs actual values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(actual.detach().numpy(), predicted.detach().numpy())\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Actual')\n",
    "plt.show()\n",
    "\n",
    "# Let's calculate how close the predicted values are to the actual values\n",
    "# If the prediction, rounded to the nearest integer is within 1e-6 of the actual value, we'll consider it correct\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(actual)):\n",
    "    if torch.abs(actual[i] - int(predicted[i])) < 1e-6:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(actual)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A New Approach\n",
    "\n",
    "The MLP classifier is not looking good after a bit of experimentation.\n",
    "Let's see if we can't now try and look at the adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1038\n",
      "[  0.  16.  -4. -28.  11.  11.  -7.   1.]\n",
      "\n",
      "Graph 1173\n",
      "[ -5.   9.  12. -28.   6.  12.  -7.   1.]\n",
      "\n",
      "Graph 988\n",
      "[ 3.60000000e+01  3.55271368e-14 -1.33000000e+02  1.00000000e+01\n",
      "  1.82000000e+02 -3.00000000e+01 -1.08000000e+02  3.00000000e+01\n",
      "  2.20000000e+01 -1.00000000e+01  1.00000000e+00]\n",
      "\n",
      "Graph 586\n",
      "[ -2.   4.  13. -16. -25.  18.  11.  -8.   1.]\n",
      "\n",
      "Graph 350\n",
      "[  8.  14. -19. -23.  18.   8.  -7.   1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The adj matrices are all different sizes, so we can't use them as input to the model\n",
    "# We'll instead see if we can use the characteristics of the adj matrix to predict the invariants\n",
    "\n",
    "# Let's get the characteristic polynomial of each adjacency matrix\n",
    "\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "# Note, the coefficients of the polynomial are in increasing order of the power of x\n",
    "# e.g. [1, 2, 3] represents 1 + 2x + 3x^2\n",
    "def get_characteristic_polynomial(adj_matrix):\n",
    "    n = adj_matrix.shape[0]\n",
    "    I = np.eye(n)\n",
    "    A = adj_matrix + I\n",
    "    return poly.polyfromroots(np.linalg.eigvals(A))\n",
    "\n",
    "\n",
    "adj_matrices = [graph['adjacency'] for graph in graphs]\n",
    "characteristic_polynomials = [get_characteristic_polynomial(adj_matrix) for adj_matrix in adj_matrices]\n",
    "\n",
    "# Let's print 5 at random\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(characteristic_polynomials))\n",
    "    print(f\"Graph {idx + 1}\")\n",
    "    print(characteristic_polynomials[idx])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7277e-34, 4.0000e+00, 1.3000e+01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 2.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0000e+00, 5.0000e+00, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [3.0000e+00, 7.0000e+00, 9.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 1.0000e+01, 1.7764e-15,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 2.])\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=11, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 0 - Loss: 0.7374446392059326\n",
      "Epoch 10 - Loss: 0.537780225276947\n",
      "Epoch 20 - Loss: 0.5374536514282227\n",
      "Epoch 30 - Loss: 0.5374162197113037\n",
      "Epoch 40 - Loss: 0.5374029874801636\n",
      "Epoch 50 - Loss: 0.537395715713501\n",
      "Epoch 60 - Loss: 0.5373952984809875\n",
      "Epoch 70 - Loss: 0.537395715713501\n",
      "Epoch 80 - Loss: 0.5373950004577637\n",
      "Epoch 90 - Loss: 0.5373937487602234\n",
      "Epoch 100 - Loss: 0.5373929738998413\n",
      "Epoch 110 - Loss: 0.5373926758766174\n",
      "Epoch 120 - Loss: 0.5373924970626831\n",
      "Epoch 130 - Loss: 0.5373923778533936\n",
      "Epoch 140 - Loss: 0.537392258644104\n",
      "Epoch 150 - Loss: 0.5373921394348145\n",
      "Epoch 160 - Loss: 0.5373920798301697\n",
      "Epoch 170 - Loss: 0.5373919606208801\n",
      "Epoch 180 - Loss: 0.5373919010162354\n",
      "Epoch 190 - Loss: 0.5373918414115906\n",
      "Epoch 200 - Loss: 0.537391722202301\n",
      "Epoch 210 - Loss: 0.5373916625976562\n",
      "Epoch 220 - Loss: 0.5373916625976562\n",
      "Epoch 230 - Loss: 0.5373916029930115\n",
      "Epoch 240 - Loss: 0.5373915433883667\n",
      "Epoch 250 - Loss: 0.5373914837837219\n",
      "Epoch 260 - Loss: 0.5373914837837219\n",
      "Epoch 270 - Loss: 0.5373914241790771\n",
      "Epoch 280 - Loss: 0.5373913645744324\n",
      "Epoch 290 - Loss: 0.5373913645744324\n",
      "Epoch 300 - Loss: 0.5373913049697876\n",
      "Epoch 310 - Loss: 0.5373912453651428\n",
      "Epoch 320 - Loss: 0.5373912453651428\n",
      "Epoch 330 - Loss: 0.537391185760498\n",
      "Epoch 340 - Loss: 0.537391185760498\n",
      "Epoch 350 - Loss: 0.537391185760498\n",
      "Epoch 360 - Loss: 0.537391185760498\n",
      "Epoch 370 - Loss: 0.537391185760498\n",
      "Epoch 380 - Loss: 0.5373911261558533\n",
      "Epoch 390 - Loss: 0.5373910665512085\n",
      "Epoch 400 - Loss: 0.5373910665512085\n",
      "Epoch 410 - Loss: 0.5373910665512085\n",
      "Epoch 420 - Loss: 0.5373910069465637\n",
      "Epoch 430 - Loss: 0.5373910069465637\n",
      "Epoch 440 - Loss: 0.5373910069465637\n",
      "Epoch 450 - Loss: 0.537390947341919\n",
      "Epoch 460 - Loss: 0.5373910069465637\n",
      "Epoch 470 - Loss: 0.5373908877372742\n",
      "Epoch 480 - Loss: 0.5373908877372742\n",
      "Epoch 490 - Loss: 0.5373908877372742\n",
      "Epoch 500 - Loss: 0.5373908877372742\n",
      "Epoch 510 - Loss: 0.5373908877372742\n",
      "Epoch 520 - Loss: 0.5373908877372742\n",
      "Epoch 530 - Loss: 0.5373908877372742\n",
      "Epoch 540 - Loss: 0.5373908877372742\n",
      "Epoch 550 - Loss: 0.5373908877372742\n",
      "Epoch 560 - Loss: 0.5373908281326294\n",
      "Epoch 570 - Loss: 0.5373908281326294\n",
      "Epoch 580 - Loss: 0.5373908281326294\n",
      "Epoch 590 - Loss: 0.5373908281326294\n",
      "Epoch 600 - Loss: 0.5373908281326294\n",
      "Epoch 610 - Loss: 0.5373908281326294\n",
      "Epoch 620 - Loss: 0.5373908281326294\n",
      "Epoch 630 - Loss: 0.5373908281326294\n",
      "Epoch 640 - Loss: 0.5373908281326294\n",
      "Epoch 650 - Loss: 0.5373908281326294\n",
      "Epoch 660 - Loss: 0.5373908281326294\n",
      "Epoch 670 - Loss: 0.5373907685279846\n",
      "Epoch 680 - Loss: 0.5373907685279846\n",
      "Epoch 690 - Loss: 0.5373907685279846\n",
      "Epoch 700 - Loss: 0.5373907685279846\n",
      "Epoch 710 - Loss: 0.5373907685279846\n",
      "Epoch 720 - Loss: 0.5373907685279846\n",
      "Epoch 730 - Loss: 0.5373907685279846\n",
      "Epoch 740 - Loss: 0.5373907685279846\n",
      "Epoch 750 - Loss: 0.5373907685279846\n",
      "Epoch 760 - Loss: 0.5373907685279846\n",
      "Epoch 770 - Loss: 0.5373907089233398\n",
      "Epoch 780 - Loss: 0.5373907685279846\n",
      "Epoch 790 - Loss: 0.5373907685279846\n",
      "Epoch 800 - Loss: 0.5373907685279846\n",
      "Epoch 810 - Loss: 0.5373906493186951\n",
      "Epoch 820 - Loss: 0.5373907685279846\n",
      "Epoch 830 - Loss: 0.5373907685279846\n",
      "Epoch 840 - Loss: 0.5373907685279846\n",
      "Epoch 850 - Loss: 0.5373907685279846\n",
      "Epoch 860 - Loss: 0.5373906493186951\n",
      "Epoch 870 - Loss: 0.5373906493186951\n",
      "Epoch 880 - Loss: 0.5373907089233398\n",
      "Epoch 890 - Loss: 0.5373906493186951\n",
      "Epoch 900 - Loss: 0.5373906493186951\n",
      "Epoch 910 - Loss: 0.5373906493186951\n",
      "Epoch 920 - Loss: 0.5373906493186951\n",
      "Epoch 930 - Loss: 0.5373907089233398\n",
      "Epoch 940 - Loss: 0.5373906493186951\n",
      "Epoch 950 - Loss: 0.5373906493186951\n",
      "Epoch 960 - Loss: 0.5373906493186951\n",
      "Epoch 970 - Loss: 0.5373906493186951\n",
      "Epoch 980 - Loss: 0.5373906493186951\n",
      "Epoch 990 - Loss: 0.5373906493186951\n",
      "Predicting for input: tensor([2.0000e+00, 9.7700e-15, 2.3000e+01, 1.2000e+01, 3.7000e+01, 2.8000e+01,\n",
      "        7.0000e+00, 8.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "Expected output: 1.0\n",
      "Predicted output: tensor([0.6685], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's use the coefficients of the characteristic polynomial as input to the model\n",
    "\n",
    "# The lengths of the polynomials are all different, so we'll need to pad them to the same length\n",
    "\n",
    "max_degree = max(len(poly) for poly in characteristic_polynomials)\n",
    "for i, poly in enumerate(characteristic_polynomials):\n",
    "    characteristic_polynomials[i] = np.pad(poly, (0, max_degree - len(poly)))\n",
    "\n",
    "# There's two things we need to do now:\n",
    "# 1. make sure all values are real by taking the magnitude of the complex numbers\n",
    "# 2. convert the list to a singlle numpy array before converting to a tensor\n",
    "\n",
    "characteristic_polynomials = np.abs(np.array(characteristic_polynomials))\n",
    "\n",
    "X = torch.tensor(characteristic_polynomials, dtype=torch.float32)\n",
    "y = torch.stack([graph['invariants'][0] for graph in graphs])\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# Let's split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "# Let's make a new model\n",
    "\n",
    "NUM_FEATURES = X.shape[1]\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(NUM_FEATURES, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, NUM_CLASSES)\n",
    "        self.out = torch.nn.Linear(NUM_CLASSES, 1)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.nn.Tanh()(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We'll use a sigmoid activation function for the hidden layers\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self):\n",
    "        torch.save(self.state_dict(), 'mlp.dat')\n",
    "\n",
    "    def load_weights(self):\n",
    "        self.load_state_dict(torch.load('mlp.dat'))\n",
    "\n",
    "\n",
    "model = MLP()\n",
    "print(model)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n",
    "\n",
    "model.save_weights()\n",
    "\n",
    "random_idx = np.random.randint(0, len(X_test))\n",
    "test_input = X_test[random_idx]\n",
    "\n",
    "print(f\"Predicting for input: {test_input}\")\n",
    "print(f\"Expected output: {y_test[random_idx]}\")\n",
    "print(f\"Predicted output: {model(test_input)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
